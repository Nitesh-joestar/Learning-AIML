{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2997685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nichu\\.conda\\envs\\jupytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab96769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 face(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"person (2).jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Detect faces\n",
    "faces = RetinaFace.detect_faces(image_path)  # returns a dict\n",
    "\n",
    "if faces == False:\n",
    "    print(\"No faces detected\")\n",
    "else:\n",
    "    print(f\"Detected {len(faces)} face(s)\")\n",
    "    for key, face in faces.items():\n",
    "        bbox = face['facial_area']   # [x1, y1, x2, y2]\n",
    "        landmarks = face['landmarks']  # left_eye, right_eye, nose, mouth_left, mouth_right\n",
    "\n",
    "        # Draw bounding box\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw landmarks\n",
    "        for _, point in landmarks.items():\n",
    "            point = tuple(map(int, point))  # convert floats to int\n",
    "            cv2.circle(image, point, 2, (0, 0, 255), -1)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a04ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 face(s)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "# --- 1. Load Image ---\n",
    "image_path = \"person (1).jpg\"\n",
    "try:\n",
    "    image = cv2.imread(image_path)\n",
    "    # Make a copy for drawing on, so the original remains clean for cropping\n",
    "    image_with_detections = image.copy()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Detect Faces ---\n",
    "# Pass the loaded image object directly for better efficiency\n",
    "try:\n",
    "    faces = RetinaFace.detect_faces(image)\n",
    "except Exception as e:\n",
    "    print(f\"Error during face detection: {e}\")\n",
    "    # This can happen if the model fails to load, etc.\n",
    "    faces = {}\n",
    "\n",
    "\n",
    "# --- 3. Process Detections ---\n",
    "# Use the correct, Pythonic way to check if the dictionary is empty\n",
    "if not faces:\n",
    "    print(\"No faces detected in the image.\")\n",
    "else:\n",
    "    print(f\"Detected {len(faces)} face(s)\")\n",
    "    \n",
    "    # Enumerate to get a simple face counter for window titles\n",
    "    for i, (key, face) in enumerate(faces.items()):\n",
    "        \n",
    "        # --- Extract Data ---\n",
    "        bbox = face['facial_area']      # Bounding box [x1, y1, x2, y2]\n",
    "        landmarks = face['landmarks']   # Key points on the face\n",
    "        \n",
    "        # Unpack bounding box coordinates\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # --- a) Draw on the copy of the image ---\n",
    "        # Draw bounding box (green)\n",
    "        cv2.rectangle(image_with_detections, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw landmarks (red)\n",
    "        for _, point in landmarks.items():\n",
    "            point = tuple(map(int, point))  # Convert float coordinates to int\n",
    "            cv2.circle(image_with_detections, point, 2, (0, 0, 255), -1)\n",
    "\n",
    "        # --- b) Crop the face from the ORIGINAL image ---\n",
    "        # Use NumPy slicing to crop the face using the bounding box\n",
    "        # Add a small buffer/padding to avoid cutting off edges\n",
    "        pad = 10\n",
    "        cropped_face = image[max(0, y1-pad):min(y2+pad, image.shape[0]), \n",
    "                              max(0, x1-pad):min(x2+pad, image.shape[1])]\n",
    "        \n",
    "        # Display the cropped face in a new window\n",
    "        if cropped_face.size > 0:\n",
    "            cv2.imshow(f\"Cropped Face {i+1}\", cropped_face)\n",
    "\n",
    "    # --- 4. Show the final result with all detections ---\n",
    "    cv2.imshow(\"All Detected Faces\", image_with_detections)\n",
    "\n",
    "\n",
    "# --- 5. Wait for user input and clean up ---\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b895fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    cv2.imshow(\"Img\",image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cba6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(cropped_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4957314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
