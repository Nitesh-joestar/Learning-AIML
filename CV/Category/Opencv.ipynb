{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec2c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import tkinter as tk\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9a02c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nichu\\.conda\\envs\\jupytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffdb5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=cv.imread('../Data/Images/cat (1).jpeg',cv.IMREAD_REDUCED_COLOR_2)\n",
    "img2=cv.imread('../Data/Images/cat (2).jpeg',cv.IMREAD_REDUCED_COLOR_2)\n",
    "img3=cv.imread('../Data/Images/cat (3).jpeg',cv.IMREAD_REDUCED_COLOR_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6047a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img,name=None):\n",
    "    window_name=name if name else \"Default\"\n",
    "    cv.imshow(window_name,img)\n",
    "    cv.waitKey()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "649de8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86919cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imwrite('catcute.jpeg',img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebd85e",
   "metadata": {},
   "source": [
    "Differnet image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c50478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load a color image (ignores alpha channel)\n",
    "img_color = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_COLOR)        # or cv.imread(\"../Data/Images/cat (1).jpeg\", 1)\n",
    "\n",
    "# 2. Load a grayscale image\n",
    "img_gray = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_GRAYSCALE)     # or cv.imread(\"../Data/Images/cat (1).jpeg\", 0)\n",
    "\n",
    "# 3. Load the image as-is (including alpha channel if present)\n",
    "img_unchanged = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_UNCHANGED)  # or cv.imread(\"../Data/Images/cat (1).jpeg\", -1)\n",
    "\n",
    "# 4. Load image preserving bit depth\n",
    "img_anydepth = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_ANYDEPTH)   # 16-bit, 32-bit images\n",
    "\n",
    "# 5. Load image in any color format\n",
    "img_anycolor = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_ANYCOLOR)\n",
    "\n",
    "# 6. Load reduced-size grayscale images\n",
    "img_gray_half = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_GRAYSCALE_2)\n",
    "img_gray_quarter = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_GRAYSCALE_4)\n",
    "img_gray_eighth = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_GRAYSCALE_8)\n",
    "\n",
    "# 7. Load reduced-size color images\n",
    "img_color_half = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_COLOR_2)\n",
    "img_color_quarter = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_COLOR_4)\n",
    "img_color_eighth = cv.imread(\"../Data/Images/cat (1).jpeg\", cv.IMREAD_REDUCED_COLOR_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d7238e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img_color_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "867a2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b6c2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0e86935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(video_path, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Plays a video scaled to fit your screen with correct FPS.\n",
    "    \n",
    "    :param video_path: Path to the video file\n",
    "    :param skip_frames: Show every Nth frame (1 = show all)\n",
    "    \"\"\"\n",
    "    # Get screen resolution using Tkinter\n",
    "    root = tk.Tk()\n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    screen_height = root.winfo_screenheight()\n",
    "    root.destroy()\n",
    "\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    fps = cap.get(cv.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"FPS: {fps}, Total frames: {total_frames}, Screen: {screen_width}x{screen_height}\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip frames if needed\n",
    "        current_frame = int(cap.get(cv.CAP_PROP_POS_FRAMES))\n",
    "        if current_frame % skip_frames != 0:\n",
    "            continue\n",
    "\n",
    "        # Resize frame to fit screen\n",
    "        h, w = frame.shape[:2]\n",
    "        scale = min(screen_width / w, screen_height / h, 1.0)\n",
    "        if scale < 1.0:\n",
    "            frame = cv.resize(frame, (int(w * scale), int(h * scale)))\n",
    "\n",
    "        cv.imshow(\"Video\", frame)\n",
    "        if cv.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4b4ff",
   "metadata": {},
   "source": [
    "Cool kernels exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f247e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filters with descriptive info\n",
    "filters = {\n",
    "    \"Blur\": {\n",
    "        \"func\": lambda x: cv.blur(x, (5,5)),\n",
    "        \"desc\": \"Simple average blur (5x5 kernel)\"\n",
    "    },\n",
    "    \"GaussianBlur\": {\n",
    "        \"func\": lambda x: cv.GaussianBlur(x, (5,5), 0),\n",
    "        \"desc\": \"Gaussian blur (5x5 kernel, sigma auto)\"\n",
    "    },\n",
    "    \"MedianBlur\": {\n",
    "        \"func\": lambda x: cv.medianBlur(x, 5),\n",
    "        \"desc\": \"Median blur (removes salt-and-pepper noise)\"\n",
    "    },\n",
    "    \"BilateralFilter\": {\n",
    "        \"func\": lambda x: cv.bilateralFilter(x, 9, 75, 75),\n",
    "        \"desc\": \"Edge-preserving bilateral filter\"\n",
    "    },\n",
    "    \"SobelX\": {\n",
    "        \"func\": lambda x: cv.Sobel(x, cv.CV_64F, 1, 0, ksize=3),\n",
    "        \"desc\": \"Sobel gradient in X direction (vertical edges)\"\n",
    "    },\n",
    "    \"SobelY\": {\n",
    "        \"func\": lambda x: cv.Sobel(x, cv.CV_64F, 0, 1, ksize=3),\n",
    "        \"desc\": \"Sobel gradient in Y direction (horizontal edges)\"\n",
    "    },\n",
    "    \"Laplacian\": {\n",
    "        \"func\": lambda x: cv.Laplacian(x, cv.CV_64F),\n",
    "        \"desc\": \"Laplacian (second-order derivative, detects all edges)\"\n",
    "    },\n",
    "    \"Canny\": {\n",
    "        \"func\": lambda x: cv.Canny(x, 100, 200),\n",
    "        \"desc\": \"Canny edge detector (binary edges)\"\n",
    "    }\n",
    "}\n",
    "other_filters = {\n",
    "    \"DetailEnhance\": {\n",
    "        \"func\": lambda x: cv.detailEnhance(x, sigma_s=10, sigma_r=0.15),\n",
    "        \"desc\": \"Enhances details and local contrast for a punchy look\"\n",
    "    },\n",
    "    \"Stylization\": {\n",
    "        \"func\": lambda x: cv.stylization(x, sigma_s=150, sigma_r=0.25),\n",
    "        \"desc\": \"Turns image into a cartoon/painterly style\"\n",
    "    },\n",
    "    \"PencilSketchGray\": {\n",
    "        \"func\": lambda x: cv.pencilSketch(x, sigma_s=60, sigma_r=0.07, shade_factor=0.05)[0],\n",
    "        \"desc\": \"Grayscale pencil sketch effect\"\n",
    "    },\n",
    "    \"PencilSketchColor\": {\n",
    "        \"func\": lambda x: cv.pencilSketch(x, sigma_s=60, sigma_r=0.07, shade_factor=0.05)[1],\n",
    "        \"desc\": \"Colored pencil sketch effect\"\n",
    "    },\n",
    "    \"Erosion\": {\n",
    "        \"func\": lambda x: cv.erode(cv.cvtColor(x, cv.COLOR_BGR2GRAY), np.ones((3,3), np.uint8), iterations=1),\n",
    "        \"desc\": \"Erodes bright regions (shrinks shapes)\"\n",
    "    },\n",
    "    \"Dilation\": {\n",
    "        \"func\": lambda x: cv.dilate(cv.cvtColor(x, cv.COLOR_BGR2GRAY), np.ones((3,3), np.uint8), iterations=1),\n",
    "        \"desc\": \"Dilates bright regions (expands shapes)\"\n",
    "    },\n",
    "    \"Opening\": {\n",
    "        \"func\": lambda x: cv.morphologyEx(cv.cvtColor(x, cv.COLOR_BGR2GRAY), cv.MORPH_OPEN, np.ones((3,3), np.uint8)),\n",
    "        \"desc\": \"Erosion followed by dilation (removes small noise)\"\n",
    "    },\n",
    "    \"Closing\": {\n",
    "        \"func\": lambda x: cv.morphologyEx(cv.cvtColor(x, cv.COLOR_BGR2GRAY), cv.MORPH_CLOSE, np.ones((3,3), np.uint8)),\n",
    "        \"desc\": \"Dilation followed by erosion (fills small holes)\"\n",
    "    },\n",
    "    \"MorphGradient\": {\n",
    "        \"func\": lambda x: cv.morphologyEx(cv.cvtColor(x, cv.COLOR_BGR2GRAY), cv.MORPH_GRADIENT, np.ones((3,3), np.uint8)),\n",
    "        \"desc\": \"Difference between dilation and erosion (highlights object boundaries)\"\n",
    "    },\n",
    "    \"AdaptiveThresholdMean\": {\n",
    "        \"func\": lambda x: cv.adaptiveThreshold(cv.cvtColor(x, cv.COLOR_BGR2GRAY), 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2),\n",
    "        \"desc\": \"Adaptive threshold using neighborhood mean\"\n",
    "    },\n",
    "    \"AdaptiveThresholdGaussian\": {\n",
    "        \"func\": lambda x: cv.adaptiveThreshold(cv.cvtColor(x, cv.COLOR_BGR2GRAY), 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2),\n",
    "        \"desc\": \"Adaptive threshold using Gaussian-weighted sum\"\n",
    "    },\n",
    "    \"OtsuThreshold\": {\n",
    "        \"func\": lambda x: cv.threshold(cv.cvtColor(x, cv.COLOR_BGR2GRAY), 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)[1],\n",
    "        \"desc\": \"Automatically computes optimal global threshold\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def filter(img):\n",
    "    for name, info in other_filters.items():\n",
    "        filtered = info[\"func\"](img)\n",
    "        \n",
    "        # Convert float images to uint8 for display\n",
    "        if filtered.dtype != np.uint8:\n",
    "            filtered = cv.convertScaleAbs(filtered)\n",
    "        \n",
    "        # Show image with descriptive window title\n",
    "        window_title = f\"{info['desc']}\"\n",
    "        cv.imshow(window_title, filtered)\n",
    "        cv.waitKey(0)\n",
    "    \n",
    "    cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af3b97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb069309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(img):\n",
    "    \"\"\"\n",
    "    img: already-loaded image (BGR) as a NumPy array\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        print(\"Image not found!\")\n",
    "        return\n",
    "\n",
    "    # Convert to grayscale (most detectors require this)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Define feature detectors\n",
    "    detectors = {\n",
    "        \"SIFT\": cv.SIFT_create(),\n",
    "        \"ORB\": cv.ORB_create(),\n",
    "        \"BRISK\": cv.BRISK_create(),\n",
    "        \"AKAZE\": cv.AKAZE_create(),\n",
    "        \"FAST\": cv.FastFeatureDetector_create(),\n",
    "        \"KAZE\": cv.KAZE_create()\n",
    "    }\n",
    "\n",
    "    for name, detector in detectors.items():\n",
    "        if detector is None:\n",
    "            continue  # Skip if detector not available\n",
    "        if name in [\"FAST\"]:\n",
    "            # FAST only detects keypoints, no descriptors\n",
    "            keypoints = detector.detect(gray, None)\n",
    "        else:\n",
    "            keypoints, _ = detector.detectAndCompute(gray, None)\n",
    "\n",
    "        # Draw keypoints\n",
    "        img_kp = cv.drawKeypoints(img, keypoints, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "        # Show result\n",
    "        cv.imshow(f\"{name} Keypoints - Total: {len(keypoints)}\", img_kp)\n",
    "        cv.waitKey(0)\n",
    "\n",
    "    cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed499e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_features(img1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupytorch",
   "language": "python",
   "name": "jupytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
